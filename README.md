# IoT ì„¼ì„œ ëª¨ë‹ˆí„°ë§ íŒŒì´í”„ë¼ì¸ - ì¸ìˆ˜ì¸ê³„ ë¬¸ì„œ

> **ì‘ì„±ì¼**: 2025-12-11
> **ëŒ€ìƒ**: ë°±ì—”ë“œ ê°œë°œì (Python/FastAPI ê²½í—˜ì, ë¹…ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹ ê·œ í•™ìŠµì)
> **ëª©ì **: í”„ë¡œì íŠ¸ ì „ì²´ êµ¬ì¡° ì´í•´ ë° ë¡œì»¬ í™˜ê²½ êµ¬ì¶•Â·ìš´ì˜ ê°€ì´ë“œ

---

## ğŸ“‘ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì•„í‚¤í…ì²˜](#2-ì•„í‚¤í…ì²˜)
3. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#3-ë””ë ‰í† ë¦¬-êµ¬ì¡°)
4. [ë¡œì»¬ í™˜ê²½ êµ¬ì¶•](#4-ë¡œì»¬-í™˜ê²½-êµ¬ì¶•)
5. [ì‹œìŠ¤í…œ ìš´ì˜](#5-ì‹œìŠ¤í…œ-ìš´ì˜)
6. [ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰](#6-ë°ì´í„°-íŒŒì´í”„ë¼ì¸-ì‹¤í–‰)
7. [API ì„œë²„ ì‚¬ìš©](#7-api-ì„œë²„-ì‚¬ìš©)
8. [í™•ì¥ ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•](#8-í™•ì¥-ë°-ì»¤ìŠ¤í„°ë§ˆì´ì§•)
9. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#9-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
10. [ê¸°ìˆ  ìŠ¤íƒ ìƒì„¸](#10-ê¸°ìˆ -ìŠ¤íƒ-ìƒì„¸)
11. [ì°¸ê³  ìë£Œ](#11-ì°¸ê³ -ìë£Œ)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 í”„ë¡œì íŠ¸ ëª©ì 

IoT ì„¼ì„œ ë°ì´í„°(ì˜¨ë„, ìŠµë„, ì••ë ¥)ë¥¼ **ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘Â·ì²˜ë¦¬Â·ì €ì¥**í•˜ê³ , **REST APIì™€ ì›¹ UI**ë¥¼ í†µí•´ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

**ì£¼ìš” ê¸°ëŠ¥**:
- ì„¼ì„œ ë°ì´í„° 0.5ì´ˆ ê°„ê²© ì‹¤ì‹œê°„ ìƒì„± (Python Generator)
- Kafkaë¥¼ í†µí•œ ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
- Sparkë¥¼ í†µí•œ ì‹œê°„ë³„ ë°°ì¹˜ ì§‘ê³„ ì²˜ë¦¬
- Apache Iceberg í…Œì´ë¸”ë¡œ ë°ì´í„° ì €ì¥ (MinIO S3 + PostgreSQL ì¹´íƒˆë¡œê·¸)
- FastAPI ê¸°ë°˜ REST API ì„œë²„ë¡œ Spark í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ ë° Job ì œì¶œ
- React ê¸°ë°˜ ì›¹ UIë¡œ ì‹œê°í™”

### 1.2 ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì„±ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Pipeline                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Python Generator]  â”€â”€â”€â”€â”€â–¶  [Kafka: sensor-raw]
   (ì„¼ì„œ ë°ì´í„° ìƒì„±)            (ë©”ì‹œì§€ ë¸Œë¡œì»¤)
   0.5ì´ˆ ê°„ê²©                    â”‚
                                 â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚                   â”‚
                       â–¼                   â–¼
                  [Spark]              [Flink]
                ë°°ì¹˜ ì²˜ë¦¬              (ë¯¸ì‚¬ìš©)
              ì‹œê°„ë³„ ì§‘ê³„
                       â”‚
                       â–¼
              [MinIO S3 Storage]
           Apache Iceberg Tables
              (Parquet í¬ë§·)
                       â”‚
                       â–¼
              [PostgreSQL]
           - Iceberg ì¹´íƒˆë¡œê·¸
           - ì§‘ê³„ ê²°ê³¼ ì €ì¥


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Management & Monitoring                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[spark-fastapi]  â—€â”€â”€â”€â”€â–¶  [Spark Cluster]
  FastAPI REST API         (Master + Workers)
        â”‚
        â”‚ HTTP
        â–¼
[spark-fastapi-ui]
  React Web UI
```

### 1.3 ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ìš”ì•½

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ê¸°ìˆ  ìŠ¤íƒ |
|---------|------|----------|
| **iot-pipeline/generator** | ì„¼ì„œ ë°ì´í„° ìƒì„±ê¸° | Python 3.11, kafka-python |
| **iot-pipeline/spark-jobs** | ë°°ì¹˜ ë°ì´í„° ì§‘ê³„ | PySpark 3.5.3, Apache Iceberg |
| **etl-cluster** | ì¸í”„ë¼ í™˜ê²½ | Docker Compose (Kafka, Spark, PostgreSQL, MinIO) |
| **spark-fastapi** | Spark í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ API | FastAPI, httpx, Pydantic |
| **spark-fastapi-ui** | ì›¹ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ | React, TypeScript, Vite |

---

## 2. ì•„í‚¤í…ì²˜

### 2.1 ë°ì´í„° í”Œë¡œìš°

#### ğŸ“Š ì „ì²´ ë°ì´í„° íë¦„

```
1. ë°ì´í„° ìƒì„±
   â””â”€â–¶ Python Generatorê°€ ì„¼ì„œ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ìƒì„±
       (ì˜¨ë„, ìŠµë„, ì••ë ¥ ì„¼ì„œ 3ì¢…ë¥˜)

2. ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°
   â””â”€â–¶ Kafka Topic(sensor-raw)ì— Publish
       - ë©”ì‹œì§€ í¬ë§·: JSON
       - íŒŒí‹°ì…˜: 1ê°œ (í…ŒìŠ¤íŠ¸ í™˜ê²½)

3. ë°°ì¹˜ ì²˜ë¦¬
   â””â”€â–¶ Spark Jobì´ Kafka Topicì—ì„œ ë°ì´í„° ì½ê¸°
       - ì‹œê°„ë³„(hourly) ì§‘ê³„: AVG, MIN, MAX, COUNT
       - ì„¼ì„œë³„, ìœ„ì¹˜ë³„ ê·¸ë£¹í•‘

4. ë°ì´í„° ì €ì¥
   â””â”€â–¶ Apache Iceberg í…Œì´ë¸”ì— ì €ì¥
       - íŒŒì¼ í¬ë§·: Parquet (ì»¬ëŸ¼í˜• ì••ì¶•)
       - ìŠ¤í† ë¦¬ì§€: MinIO (S3 í˜¸í™˜)
       - ë©”íƒ€ë°ì´í„°: PostgreSQL JDBC ì¹´íƒˆë¡œê·¸

5. ì¡°íšŒ ë° ëª¨ë‹ˆí„°ë§
   â””â”€â–¶ Spark SQLë¡œ Iceberg í…Œì´ë¸” ì¿¼ë¦¬
       â””â”€â–¶ FastAPIë¡œ Spark í´ëŸ¬ìŠ¤í„° ìƒíƒœ ëª¨ë‹ˆí„°ë§
           â””â”€â–¶ React UIë¡œ ì‹œê°í™”
```

### 2.2 ì»´í¬ë„ŒíŠ¸ë³„ ì—­í• 

#### ğŸ”¹ Apache Kafka (ë©”ì‹œì§€ ë¸Œë¡œì»¤)
**ì™œ í•„ìš”í•œê°€?**
ì„¼ì„œ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘Â·ì „ì†¡í•˜ë ¤ë©´ ìƒì„±ì(Producer)ì™€ ì†Œë¹„ì(Consumer)ë¥¼ ë¶„ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. KafkaëŠ” ì´ ì¤‘ê°„ì—ì„œ ë©”ì‹œì§€ë¥¼ ë²„í¼ë§í•˜ê³ , ì—¬ëŸ¬ Consumer(Spark, Flink ë“±)ê°€ ë…ë¦½ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•**:
- Topic ê¸°ë°˜ Pub/Sub ëª¨ë¸
- ë†’ì€ ì²˜ë¦¬ëŸ‰ (ìˆ˜ë§Œ ë©”ì‹œì§€/ì´ˆ)
- ë©”ì‹œì§€ ì˜ì†ì„± (ë””ìŠ¤í¬ ì €ì¥)
- KRaft ëª¨ë“œ ì‚¬ìš© (Zookeeper ë¶ˆí•„ìš”)

**ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• **:
- `sensor-raw` Topicì— ì„¼ì„œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
- Spark Consumerê°€ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì½ê¸°

#### ğŸ”¹ Apache Spark (ë¶„ì‚° ì²˜ë¦¬ ì—”ì§„)
**ì™œ í•„ìš”í•œê°€?**
ëŒ€ëŸ‰ì˜ ì„¼ì„œ ë°ì´í„°ë¥¼ ì‹œê°„ë³„ë¡œ ì§‘ê³„í•˜ë ¤ë©´ ë‹¨ì¼ ì„œë²„ë¡œëŠ” ì²˜ë¦¬ ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤. SparkëŠ” ë°ì´í„°ë¥¼ ì—¬ëŸ¬ Workerì— ë¶„ì‚°ì‹œì¼œ ë³‘ë ¬ ì²˜ë¦¬í•˜ë©°, ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ë¹ ë¥¸ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•**:
- ì¸ë©”ëª¨ë¦¬ ë¶„ì‚° ì²˜ë¦¬ (MapReduceë³´ë‹¤ ë¹ ë¦„)
- Structured Streaming / Batch ì²˜ë¦¬ ì§€ì›
- Scala, Python, Java API ì œê³µ

**ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• **:
- Kafka Topicì—ì„œ ì„¼ì„œ ë°ì´í„° ì½ê¸°
- ì‹œê°„ë³„(hourly) ì§‘ê³„ ì¿¼ë¦¬ ì‹¤í–‰
- Iceberg í…Œì´ë¸”ì— ê²°ê³¼ ì €ì¥

**Standalone í´ëŸ¬ìŠ¤í„° êµ¬ì„±**:
- **Master**: ìŠ¤ì¼€ì¤„ë§ ë° Worker ê´€ë¦¬
- **Worker**: ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ìˆ˜í–‰
- **Driver**: Job ì œì¶œ ë° ì‹¤í–‰ ê³„íš ìˆ˜ë¦½

#### ğŸ”¹ Apache Iceberg (í…Œì´ë¸” í¬ë§·)
**ì™œ í•„ìš”í•œê°€?**
Parquet íŒŒì¼ì„ ì§ì ‘ ë‹¤ë£¨ë©´ ìŠ¤í‚¤ë§ˆ ë³€ê²½, íŒŒí‹°ì…˜ ê´€ë¦¬, ACID íŠ¸ëœì­ì…˜ì´ ì–´ë µìŠµë‹ˆë‹¤. IcebergëŠ” ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì˜¤í”ˆ í…Œì´ë¸” í¬ë§·ìœ¼ë¡œ, **ë°ì´í„° ë ˆì´í¬ë¥¼ ë§ˆì¹˜ DBì²˜ëŸ¼** ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•**:
- ìŠ¤í‚¤ë§ˆ ì§„í™” (Schema Evolution)
- Hidden Partitioning (íŒŒí‹°ì…˜ ìë™ ê´€ë¦¬)
- Time Travel (ê³¼ê±° ìŠ¤ëƒ…ìƒ· ì¡°íšŒ)
- ACID íŠ¸ëœì­ì…˜

**ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• **:
- Spark ì§‘ê³„ ê²°ê³¼ë¥¼ `iot.hourly_stats` í…Œì´ë¸”ë¡œ ì €ì¥
- PostgreSQLì´ ë©”íƒ€ë°ì´í„°(ìŠ¤í‚¤ë§ˆ, íŒŒí‹°ì…˜ ì •ë³´) ê´€ë¦¬
- MinIOê°€ ì‹¤ì œ Parquet ë°ì´í„° íŒŒì¼ ì €ì¥

#### ğŸ”¹ MinIO (S3 í˜¸í™˜ ìŠ¤í† ë¦¬ì§€)
**ì™œ í•„ìš”í•œê°€?**
AWS S3ëŠ” ìœ ë£Œì´ê³  ë¡œì»¬ í…ŒìŠ¤íŠ¸ê°€ ì–´ë µìŠµë‹ˆë‹¤. MinIOëŠ” S3 APIì™€ í˜¸í™˜ë˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ê°ì²´ ìŠ¤í† ë¦¬ì§€ë¡œ, ë¡œì»¬ í™˜ê²½ì—ì„œ S3ì™€ ë™ì¼í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.

**ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• **:
- `warehouse/` ë²„í‚·ì— Iceberg Parquet íŒŒì¼ ì €ì¥
- Sparkê°€ s3a í”„ë¡œí† ì½œë¡œ ì ‘ê·¼

#### ğŸ”¹ PostgreSQL (ë©”íƒ€ë°ì´í„° ì¹´íƒˆë¡œê·¸)
**ì™œ í•„ìš”í•œê°€?**
IcebergëŠ” í…Œì´ë¸” ë©”íƒ€ë°ì´í„°(ìŠ¤í‚¤ë§ˆ, íŒŒí‹°ì…˜, ìŠ¤ëƒ…ìƒ· ì´ë ¥)ë¥¼ ì–´ë”˜ê°€ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤. PostgreSQL JDBC ì¹´íƒˆë¡œê·¸ëŠ” ì´ ë©”íƒ€ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ê³ , ì—¬ëŸ¬ Spark Jobì´ ë™ì‹œì— í…Œì´ë¸”ì„ ì•ˆì „í•˜ê²Œ ì½ê³  ì“¸ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

**ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• **:
- Iceberg ì¹´íƒˆë¡œê·¸ ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥ (`iceberg_tables`, `iceberg_namespace_properties` í…Œì´ë¸”)
- ì‹¤ì œ ì„¼ì„œ ì§‘ê³„ ë°ì´í„°ëŠ” MinIOì— Parquet íŒŒì¼ë¡œ ì €ì¥

#### ğŸ”¹ spark-fastapi (ê´€ë¦¬ API ì„œë²„)
**ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• **:
- Spark Master REST APIë¥¼ Wrappingí•˜ì—¬ ì‚¬ìš©ì ì¹œí™”ì ì¸ API ì œê³µ
- í´ëŸ¬ìŠ¤í„° ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ì½”ì–´, ë©”ëª¨ë¦¬, Worker, ì• í”Œë¦¬ì¼€ì´ì…˜)
- Spark Job ì œì¶œ ë° ì‹¤í–‰ ì¤‘ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ
- CORS ì„¤ì •ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì™€ í†µì‹ 

**ì£¼ìš” API ì—”ë“œí¬ì¸íŠ¸**:
- `GET /api/v1/cluster/status` - í´ëŸ¬ìŠ¤í„° ìƒíƒœ
- `GET /api/v1/cluster/workers` - Worker ì •ë³´
- `GET /api/v1/jobs/apps` - ì• í”Œë¦¬ì¼€ì´ì…˜ ëª©ë¡
- `POST /api/v1/jobs/submit` - Job ì œì¶œ
- `DELETE /api/v1/jobs/apps/{app_id}` - ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ

#### ğŸ”¹ spark-fastapi-ui (ì›¹ ëŒ€ì‹œë³´ë“œ)
**ì´ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• **:
- spark-fastapi APIë¥¼ í†µí•´ Spark í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
- í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ì‹¤í–‰ ì¤‘ì¸ Job í™•ì¸ ë° ê´€ë¦¬

### 2.3 ë„¤íŠ¸ì›Œí¬ ë° í¬íŠ¸ êµ¬ì„±

| ì„œë¹„ìŠ¤ | í¬íŠ¸ | ìš©ë„ |
|--------|------|------|
| Kafka Broker | 9092 (ì™¸ë¶€), 29092 (ë‚´ë¶€) | Producer/Consumer ì—°ê²° |
| Kafka UI | 9090 | Kafka í† í”½ ëª¨ë‹ˆí„°ë§ ì›¹ UI |
| Spark Master UI | 8080 | í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸ |
| Spark Master | 7077 | Spark Job ì œì¶œ |
| PostgreSQL | 5432 | JDBC ì—°ê²° |
| pgAdmin | 5050 | PostgreSQL ê´€ë¦¬ UI |
| MinIO API | 9000 | S3 í˜¸í™˜ API |
| MinIO Console | 9001 | MinIO ê´€ë¦¬ UI |
| spark-fastapi | 8000 | REST API ì„œë²„ |
| spark-fastapi-ui | 5173 | React ê°œë°œ ì„œë²„ |

**Docker ë„¤íŠ¸ì›Œí¬**: ëª¨ë“  ì»¨í…Œì´ë„ˆëŠ” `etl-network` ë¸Œë¦¿ì§€ ë„¤íŠ¸ì›Œí¬ë¡œ í†µì‹ í•©ë‹ˆë‹¤.

---

## 3. ë””ë ‰í† ë¦¬ êµ¬ì¡°

### 3.1 ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
etl-cluster-test/
â”œâ”€â”€ etl-cluster/              # ì¸í”„ë¼ Docker Compose íŒŒì¼
â”‚   â”œâ”€â”€ docker-compose-kafka.yaml
â”‚   â”œâ”€â”€ docker-compose-spark.yaml
â”‚   â”œâ”€â”€ docker-compose-postgresql.yaml
â”‚   â””â”€â”€ docker-compose-minio.yaml
â”‚
â”œâ”€â”€ scripts/                  # í†µí•© ì…‹ì—…/ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup.sh              # ì „ì²´ í™˜ê²½ ìë™ ì„¤ì •
â”‚   â”œâ”€â”€ teardown.sh           # ì „ì²´ í™˜ê²½ ì¢…ë£Œ
â”‚   â”œâ”€â”€ create-kafka-topic.sh # Kafka í† í”½ ìƒì„±
â”‚   â””â”€â”€ create-minio-bucket.sh # MinIO ë²„í‚· ìƒì„±
â”‚
â”œâ”€â”€ iot-pipeline/             # ë°ì´í„° íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ generator/            # ì„¼ì„œ ë°ì´í„° ìƒì„±ê¸°
â”‚   â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”‚   â””â”€â”€ sensor_producer.py
â”‚   â”œâ”€â”€ spark-jobs/           # Spark ë°ì´í„° ì²˜ë¦¬
â”‚   â”‚   â””â”€â”€ pyspark-jobs/
â”‚   â”‚       â”œâ”€â”€ batch_aggregation.py
â”‚   â”‚       â””â”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ flink-jobs/           # (ë¯¸ì‚¬ìš©)
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ init-postgresql.sql
â”‚
â”œâ”€â”€ spark-fastapi/            # FastAPI ê´€ë¦¬ ì„œë²„
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/endpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ cluster.py    # í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§
â”‚   â”‚   â”‚   â””â”€â”€ jobs.py       # Job ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ core/config.py
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ spark_client.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ e2e/
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â””â”€â”€ pytest.ini
â”‚
â”œâ”€â”€ spark-fastapi-ui/         # React ì›¹ UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ shared/api/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ TESTING.md                # í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ
â””â”€â”€ HANDOVER.md               # ì´ ë¬¸ì„œ
```

### 3.2 ì£¼ìš” íŒŒì¼ ì„¤ëª…

#### ë°ì´í„° íŒŒì´í”„ë¼ì¸

| íŒŒì¼ ê²½ë¡œ | ì„¤ëª… |
|----------|------|
| `iot-pipeline/generator/sensor_producer.py` | Kafka Producerë¡œ ì„¼ì„œ ë°ì´í„° ìƒì„± (0.5ì´ˆ ê°„ê²©) |
| `iot-pipeline/spark-jobs/pyspark-jobs/batch_aggregation.py` | Kafka â†’ Spark â†’ Iceberg ë°°ì¹˜ ì²˜ë¦¬ Job |
| `iot-pipeline/config/init-postgresql.sql` | PostgreSQL ì´ˆê¸° ìŠ¤í‚¤ë§ˆ (Iceberg ì¹´íƒˆë¡œê·¸) |

#### API ì„œë²„

| íŒŒì¼ ê²½ë¡œ | ì„¤ëª… |
|----------|------|
| `spark-fastapi/app/main.py` | FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì  |
| `spark-fastapi/app/services/spark_client.py` | Spark Master REST API í´ë¼ì´ì–¸íŠ¸ |
| `spark-fastapi/app/api/endpoints/cluster.py` | í´ëŸ¬ìŠ¤í„° ìƒíƒœ, Worker ì¡°íšŒ API |
| `spark-fastapi/app/api/endpoints/jobs.py` | Job ì œì¶œ, ì•± ëª©ë¡, ì•± ì¢…ë£Œ API |

#### ì›¹ UI

| íŒŒì¼ ê²½ë¡œ | ì„¤ëª… |
|----------|------|
| `spark-fastapi-ui/src/shared/api/spark.ts` | API í´ë¼ì´ì–¸íŠ¸ (fetch ë˜í¼) |
| `spark-fastapi-ui/src/pages/ClusterPage.tsx` | í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ í˜ì´ì§€ |
| `spark-fastapi-ui/src/pages/AppsPage.tsx` | ì• í”Œë¦¬ì¼€ì´ì…˜ ëª©ë¡ í˜ì´ì§€ |

---

## 4. ë¡œì»¬ í™˜ê²½ êµ¬ì¶•

### 4.1 ì‚¬ì „ ìš”êµ¬ì‚¬í•­

**í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´**:
- Docker (ë²„ì „ 20.10 ì´ìƒ)
- Docker Compose (ë²„ì „ 2.0 ì´ìƒ)
- Python 3.11 ì´ìƒ
- Poetry (Python íŒ¨í‚¤ì§€ ê´€ë¦¬)
- Apache Spark 3.5.3 (ë¡œì»¬ì— ì„¤ì¹˜)
- Node.js 18 ì´ìƒ (í”„ë¡ íŠ¸ì—”ë“œìš©)

**í™•ì¸ ëª…ë ¹ì–´**:
```bash
docker --version
docker compose --version
python --version
poetry --version
spark-submit --version
node --version
```

### 4.2 Apache Spark ì„¤ì¹˜

#### Spark 3.5.3 ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜

```bash
# 1. HOME ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ${HOME}

# 2. Apache Spark 3.5.3 ë‹¤ìš´ë¡œë“œ
wget https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz

# 3. ì••ì¶• í•´ì œ
tar -xzf spark-3.5.3-bin-hadoop3.tgz

# 4. ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± (spark â†’ spark-3.5.3-bin-hadoop3)
ln -s ${HOME}/spark-3.5.3-bin-hadoop3 ${HOME}/spark

# 5. ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬ (ì„ íƒ)
rm spark-3.5.3-bin-hadoop3.tgz
```

#### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`~/.bashrc` ë˜ëŠ” `~/.zshrc`ì— ë‹¤ìŒ ì¶”ê°€:

```bash
export SPARK_HOME=${HOME}/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```

ì„¤ì • ì ìš©:
```bash
source ~/.bashrc  # ë˜ëŠ” source ~/.zshrc
```

#### ì„¤ì¹˜ í™•ì¸

```bash
spark-submit --version
# Spark 3.5.3ì´ ì¶œë ¥ë˜ë©´ ì„¤ì¹˜ ì™„ë£Œ
```

### 4.3 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

#### spark-fastapi/.env
```env
SPARK_MASTER_URL=http://localhost:8080
SPARK_SUBMIT_MASTER=spark://localhost:7077
SPARK_HOME=${HOME}/spark
```

#### /etc/hosts ì„¤ì • (ë¡œì»¬ì—ì„œ Docker ì»¨í…Œì´ë„ˆ ì´ë¦„ ì ‘ê·¼)
```bash
sudo tee -a /etc/hosts <<EOF
127.0.0.1 postgresql
127.0.0.1 kafka-broker-1
127.0.0.1 spark-master
127.0.0.1 minio
EOF
```

### 4.4 ë‹¨ê³„ë³„ ì„¤ì¹˜ ê°€ì´ë“œ

#### ğŸš€ ìë™ ì„¤ì • (ê¶Œì¥)

**í•œ ë²ˆì— ëª¨ë“  í™˜ê²½ ì„¤ì •**:
```bash
cd ${HOME}/etl-cluster-test
./scripts/setup.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
- Docker ë„¤íŠ¸ì›Œí¬ ìƒì„±
- Docker ë³¼ë¥¨ ìƒì„±
- MinIO ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
- ëª¨ë“  Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (Kafka, Spark, PostgreSQL, MinIO)
- PostgreSQL ì´ˆê¸°í™” (Iceberg ì¹´íƒˆë¡œê·¸ í…Œì´ë¸” ìƒì„±)

**ë‹¤ìŒ ë‹¨ê³„**:
```bash
# 1. Kafka í† í”½ ìƒì„±
./scripts/create-kafka-topic.sh

# 2. MinIO ë²„í‚· ìƒì„±
./scripts/create-minio-bucket.sh
```

#### ğŸ“‹ ìˆ˜ë™ ì„¤ì • (ì„ íƒ)

**Step 1: Docker ë„¤íŠ¸ì›Œí¬ ë° ë³¼ë¥¨ ìƒì„±**

```bash
# ë„¤íŠ¸ì›Œí¬ ìƒì„±
docker network create etl-network

# Kafka ë³¼ë¥¨ ìƒì„±
docker volume create kafka-data-1
docker volume create kafka-secrets-1
docker volume create kafka-config-1
```

**Step 2: ì¸í”„ë¼ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ìˆœì„œ ì¤‘ìš”)**

```bash
cd etl-cluster

# 1. PostgreSQL ì‹¤í–‰ (Iceberg ì¹´íƒˆë¡œê·¸ìš©)
docker compose -f docker-compose-postgresql.yaml up -d

# 2. MinIO ì‹¤í–‰ (S3 ìŠ¤í† ë¦¬ì§€ìš©)
docker compose -f docker-compose-minio.yaml up -d

# 3. Kafka ì‹¤í–‰ (ë©”ì‹œì§€ ë¸Œë¡œì»¤)
docker compose -f docker-compose-kafka.yaml up -d

# 4. Spark í´ëŸ¬ìŠ¤í„° ì‹¤í–‰
docker compose -f docker-compose-spark.yaml up -d

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps
```

**ê¸°ëŒ€ ì¶œë ¥**: 8ê°œ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (kafka-broker-1, kafka-ui, postgresql, pgadmin, minio, spark-master, spark-worker-1)

**Step 3: PostgreSQL ì´ˆê¸°í™”**

```bash
cd ../iot-pipeline
docker exec -i postgresql psql -U etl_user -d etl_db < config/init-postgresql.sql
```

**Step 4: Kafka í† í”½ ìƒì„±**

```bash
docker exec kafka-broker-1 /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server kafka-broker-1:29092 \
  --create --topic sensor-raw \
  --partitions 1 --replication-factor 1
```

**ê²€ì¦**:
```bash
docker exec kafka-broker-1 /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server kafka-broker-1:29092 \
  --list
```

**Step 5: MinIO ë²„í‚· ìƒì„±**

1. MinIO Console ì ‘ì†: http://localhost:9001
2. ë¡œê·¸ì¸: `minioadmin` / `minioadmin123`
3. Buckets ë©”ë‰´ì—ì„œ `warehouse` ë²„í‚· ìƒì„±

ë˜ëŠ” MinIO Client ì‚¬ìš©:
```bash
docker exec minio mc alias set local http://localhost:9000 minioadmin minioadmin123
docker exec minio mc mb local/warehouse
```

#### Step 6: Python ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# ì„¼ì„œ ë°ì´í„° ìƒì„±ê¸°
cd generator
poetry install

# PySpark Job
cd ../spark-jobs/pyspark-jobs
poetry install

# FastAPI ì„œë²„
cd ../../../spark-fastapi
poetry install
```

#### Step 7: í”„ë¡ íŠ¸ì—”ë“œ ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd ../spark-fastapi-ui
npm install
```

### 4.5 ì²« ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Docker ì»¨í…Œì´ë„ˆ 8ê°œ ëª¨ë‘ `Up` ìƒíƒœ
- [ ] Kafka UI (http://localhost:9090) ì ‘ì† ê°€ëŠ¥
- [ ] Spark Master UI (http://localhost:8080) ì ‘ì† ê°€ëŠ¥
- [ ] MinIO Console (http://localhost:9001) ì ‘ì† ë° `warehouse` ë²„í‚· ì¡´ì¬
- [ ] PostgreSQL ì—°ê²° í™•ì¸:
  ```bash
  docker exec -it postgresql psql -U etl_user -d etl_db -c "\dt"
  ```
- [ ] Kafka `sensor-raw` í† í”½ ì¡´ì¬

---

## 5. ì‹œìŠ¤í…œ ìš´ì˜

### 5.1 ì‹œìŠ¤í…œ ì‹œì‘ ìˆœì„œ

**ì „ì²´ ì‹œì‘**:
```bash
cd etl-cluster

# 1. ë°ì´í„°ë² ì´ìŠ¤ ë° ìŠ¤í† ë¦¬ì§€
docker compose -f docker-compose-postgresql.yaml up -d
docker compose -f docker-compose-minio.yaml up -d

# 2. ë©”ì‹œì§€ ë¸Œë¡œì»¤
docker compose -f docker-compose-kafka.yaml up -d

# 3. ì²˜ë¦¬ ì—”ì§„
docker compose -f docker-compose-spark.yaml up -d

# 4. ì›¹ ì„œë²„ (ì„ íƒ)
cd ../spark-fastapi
./startFastApi.sh  # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰

cd ../spark-fastapi-ui
npm run dev
```

### 5.2 ì‹œìŠ¤í…œ ì¢…ë£Œ ìˆœì„œ

**ğŸš€ ìë™ ì¢…ë£Œ (ê¶Œì¥)**:
```bash
cd ${HOME}/etl-cluster-test
./scripts/teardown.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
- ëª¨ë“  Docker ì»¨í…Œì´ë„ˆ ì¢…ë£Œ
- ì„ íƒì ìœ¼ë¡œ ë³¼ë¥¨ ì‚­ì œ (y/N í”„ë¡¬í”„íŠ¸)

**ğŸ“‹ ìˆ˜ë™ ì¢…ë£Œ** (ì—­ìˆœ):
```bash
# 1. ì›¹ ì„œë²„ ì¢…ë£Œ (Ctrl+C)

# 2. ì²˜ë¦¬ ì—”ì§„ ì¢…ë£Œ
cd etl-cluster
docker compose -f docker-compose-spark.yaml down

# 3. ë©”ì‹œì§€ ë¸Œë¡œì»¤ ì¢…ë£Œ
docker compose -f docker-compose-kafka.yaml down

# 4. ë°ì´í„°ë² ì´ìŠ¤ ë° ìŠ¤í† ë¦¬ì§€ ì¢…ë£Œ
docker compose -f docker-compose-minio.yaml down
docker compose -f docker-compose-postgresql.yaml down
```

**ë°ì´í„° ì™„ì „ ì‚­ì œ** (ì£¼ì˜):
```bash
# ë³¼ë¥¨ ì‚­ì œ (Kafka ë°ì´í„° ì†ì‹¤)
docker volume rm kafka-data-1 kafka-secrets-1 kafka-config-1

# ë„¤íŠ¸ì›Œí¬ ì‚­ì œ
docker network rm etl-network
```

### 5.3 ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

| ëŒ€ì‹œë³´ë“œ | URL | ë¡œê·¸ì¸ ì •ë³´ |
|---------|-----|------------|
| Kafka UI | http://localhost:9090 | ë¶ˆí•„ìš” |
| Spark Master UI | http://localhost:8080 | ë¶ˆí•„ìš” |
| MinIO Console | http://localhost:9001 | minioadmin / minioadmin123 |
| pgAdmin | http://localhost:5050 | admin@admin.com / admin |
| FastAPI Docs | http://localhost:8000/docs | ë¶ˆí•„ìš” |
| spark-fastapi-ui | http://localhost:5173 | ë¶ˆí•„ìš” |

### 5.4 ë¡œê·¸ í™•ì¸

#### Docker ì»¨í…Œì´ë„ˆ ë¡œê·¸
```bash
# ì „ì²´ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
docker logs -f kafka-broker-1
docker logs -f spark-master
docker logs -f spark-worker-1

# ìµœê·¼ 100ì¤„ í™•ì¸
docker logs --tail 100 postgresql
```

#### Spark Job ë¡œê·¸
```bash
# PySpark Job ì‹¤í–‰ ë¡œê·¸
tail -f iot-pipeline/spark-jobs/pyspark-jobs/batch_aggregation.log
```

#### FastAPI ì„œë²„ ë¡œê·¸
```bash
# ì„œë²„ ì‹¤í–‰ í„°ë¯¸ë„ì—ì„œ ì‹¤ì‹œê°„ í™•ì¸
# ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹œ
tail -f spark-fastapi/nohup.out
```

---

## 6. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

### 6.1 ì„¼ì„œ ë°ì´í„° ìƒì„±

```bash
cd iot-pipeline/generator
poetry run python sensor_producer.py
```

**ì‹¤í–‰ ê²°ê³¼**:
```
Sent: {'sensor_id': 'TEMP-001', 'sensor_type': 'temperature', ...}
Sent: {'sensor_id': 'HUMID-001', 'sensor_type': 'humidity', ...}
Sent: {'sensor_id': 'PRESS-001', 'sensor_type': 'pressure', ...}
```

**ê²€ì¦ - Kafka UI**:
1. http://localhost:9090 ì ‘ì†
2. Topics â†’ sensor-raw í´ë¦­
3. Messages íƒ­ì—ì„œ ì‹¤ì‹œê°„ ë©”ì‹œì§€ í™•ì¸

**ê²€ì¦ - Kafka CLI**:
```bash
docker exec kafka-broker-1 /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server kafka-broker-1:29092 \
  --topic sensor-raw \
  --from-beginning \
  --max-messages 5
```

### 6.2 Spark ë°°ì¹˜ ì²˜ë¦¬ Job ì‹¤í–‰

#### ë¡œì»¬ ëª¨ë“œ ì‹¤í–‰ (ê¶Œì¥ - ë””ë²„ê¹… ì‰¬ì›€)

```bash
cd iot-pipeline/spark-jobs/pyspark-jobs
spark-submit --master local[*] batch_aggregation.py
```

#### í´ëŸ¬ìŠ¤í„° ëª¨ë“œ ì‹¤í–‰

```bash
spark-submit \
  --master spark://localhost:7077 \
  --deploy-mode client \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.hadoop:hadoop-aws:3.3.4,software.amazon.awssdk:bundle:2.21.42,org.postgresql:postgresql:42.7.3 \
  batch_aggregation.py
```

**ì£¼ìš” íŒ¨í‚¤ì§€ ì„¤ëª…**:
- `iceberg-spark-runtime`: Iceberg í…Œì´ë¸” ì½ê¸°/ì“°ê¸°
- `spark-sql-kafka`: Kafka ë°ì´í„° ì†ŒìŠ¤
- `hadoop-aws`: S3A íŒŒì¼ ì‹œìŠ¤í…œ (MinIO)
- `postgresql`: JDBC ë“œë¼ì´ë²„

**ì‹¤í–‰ ë¡œê·¸ í™•ì¸**:
```bash
tail -f batch_aggregation.log
```

**Spark UIì—ì„œ í™•ì¸**:
1. http://localhost:8080 ì ‘ì†
2. Running Applications ë˜ëŠ” Completed Applications í™•ì¸
3. Application UI í´ë¦­ â†’ Jobs, Stages, Storage íƒ­ í™•ì¸

### 6.3 Iceberg í…Œì´ë¸” ì¡°íšŒ

#### spark-sql CLI ì‹¤í–‰

```bash
spark-sql \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hadoop:hadoop-aws:3.3.4,software.amazon.awssdk:bundle:2.21.42,org.postgresql:postgresql:42.7.3 \
  --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.iceberg.type=jdbc \
  --conf spark.sql.catalog.iceberg.uri=jdbc:postgresql://postgresql:5432/etl_db \
  --conf spark.sql.catalog.iceberg.jdbc.user=etl_user \
  --conf spark.sql.catalog.iceberg.jdbc.password=etl_password \
  --conf spark.sql.catalog.iceberg.warehouse=s3a://warehouse/ \
  --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
  --conf spark.hadoop.fs.s3a.access.key=minioadmin \
  --conf spark.hadoop.fs.s3a.secret.key=minioadmin123 \
  --conf spark.hadoop.fs.s3a.path.style.access=true
```

#### SQL ì¿¼ë¦¬ ì˜ˆì œ

```sql
-- ë„¤ì„ìŠ¤í˜ì´ìŠ¤(ìŠ¤í‚¤ë§ˆ) ëª©ë¡
SHOW NAMESPACES IN iceberg;

-- í…Œì´ë¸” ëª©ë¡
SHOW TABLES IN iceberg.iot;

-- ë°ì´í„° ì¡°íšŒ
SELECT * FROM iceberg.iot.hourly_stats
ORDER BY hour DESC
LIMIT 10;

-- íŠ¹ì • ì„¼ì„œ íƒ€ì… ì§‘ê³„
SELECT
  hour,
  sensor_type,
  location,
  avg_value,
  min_value,
  max_value,
  count
FROM iceberg.iot.hourly_stats
WHERE sensor_type = 'temperature'
ORDER BY hour DESC;

-- í…Œì´ë¸” ìŠ¤ëƒ…ìƒ· ì´ë ¥ (Time Travel)
SELECT * FROM iceberg.iot.hourly_stats.snapshots;

-- í…Œì´ë¸” ì‚­ì œ (ë°ì´í„° ì™„ì „ ì‚­ì œ)
DROP TABLE IF EXISTS iceberg.iot.hourly_stats PURGE;
```

#### PostgreSQLì—ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸

```bash
docker exec -it postgresql psql -U etl_user -d etl_db
```

```sql
-- Iceberg ì¹´íƒˆë¡œê·¸ í…Œì´ë¸”
\dt iceberg.*

-- í…Œì´ë¸” ë©”íƒ€ë°ì´í„°
SELECT * FROM iceberg.iceberg_tables;

-- ìŠ¤ëƒ…ìƒ· ì •ë³´
SELECT * FROM iceberg.iceberg_namespace_properties;
```

---

## 7. API ì„œë²„ ì‚¬ìš©

### 7.1 FastAPI ì„œë²„ ì‹¤í–‰

```bash
cd spark-fastapi

# ë°©ë²• 1: ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
./startFastApi.sh

# ë°©ë²• 2: ì§ì ‘ ì‹¤í–‰
poetry run python -m app.main

# ë°©ë²• 3: Uvicorn ì§ì ‘ ì‹¤í–‰
poetry run uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

**ì‹¤í–‰ í™•ì¸**:
```bash
curl http://localhost:8000/api/v1/cluster/status
```

### 7.2 ì£¼ìš” API ì—”ë“œí¬ì¸íŠ¸

#### ğŸ“Œ í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§

**GET /api/v1/cluster/status** - í´ëŸ¬ìŠ¤í„° ìƒíƒœ
```bash
curl http://localhost:8000/api/v1/cluster/status | jq
```

ì‘ë‹µ ì˜ˆì‹œ:
```json
{
  "status": "ALIVE",
  "total_cores": 8,
  "used_cores": 0,
  "total_memory": 4096,
  "used_memory": 0,
  "worker_count": 1,
  "active_app_count": 0
}
```

**GET /api/v1/cluster/workers** - Worker ì •ë³´
```bash
curl http://localhost:8000/api/v1/cluster/workers | jq
```

ì‘ë‹µ ì˜ˆì‹œ:
```json
[
  {
    "id": "worker-20241211000000-172.18.0.5-8881",
    "host": "172.18.0.5",
    "port": 8881,
    "cores": 8,
    "memory": 4096,
    "state": "ALIVE"
  }
]
```

#### ğŸ“Œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê´€ë¦¬

**GET /api/v1/jobs/apps** - ì• í”Œë¦¬ì¼€ì´ì…˜ ëª©ë¡
```bash
# ì‹¤í–‰ ì¤‘ì¸ ì•±ë§Œ
curl http://localhost:8000/api/v1/jobs/apps | jq

# ì™„ë£Œëœ ì•± í¬í•¨
curl "http://localhost:8000/api/v1/jobs/apps?include_completed=true" | jq
```

**GET /api/v1/jobs/apps/{app_id}** - íŠ¹ì • ì•± ì¡°íšŒ
```bash
curl http://localhost:8000/api/v1/jobs/apps/app-20241211000001-0001 | jq
```

**DELETE /api/v1/jobs/apps/{app_id}** - ì•± ì¢…ë£Œ
```bash
curl -X DELETE http://localhost:8000/api/v1/jobs/apps/app-20241211000001-0001 | jq
```

ì‘ë‹µ ì˜ˆì‹œ:
```json
{
  "success": true,
  "message": "Application app-20241211000001-0001 killed successfully"
}
```

#### ğŸ“Œ Job ì œì¶œ

**POST /api/v1/jobs/submit** - Spark Job ì œì¶œ
```bash
curl -X POST http://localhost:8000/api/v1/jobs/submit \
  -H "Content-Type: application/json" \
  -d '{
    "script_path": "${HOME}/etl-cluster-test/iot-pipeline/spark-jobs/pyspark-jobs/batch_aggregation.py",
    "driver_memory": "2g",
    "executor_memory": "2g",
    "executor_cores": 2,
    "num_executors": 1
  }' | jq
```

ì‘ë‹µ ì˜ˆì‹œ:
```json
{
  "success": true,
  "message": "Spark job submitted with PID: 12345"
}
```

### 7.3 Swagger UI ì‚¬ìš©

1. http://localhost:8000/docs ì ‘ì†
2. ê° ì—”ë“œí¬ì¸íŠ¸ í™•ì¥
3. "Try it out" ë²„íŠ¼ í´ë¦­
4. íŒŒë¼ë¯¸í„° ì…ë ¥ í›„ "Execute" ì‹¤í–‰
5. ì‘ë‹µ í™•ì¸

### 7.4 ì›¹ UI ì‹¤í–‰

```bash
cd spark-fastapi-ui
npm run dev
```

ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5173 ì ‘ì†

**ì£¼ìš” í˜ì´ì§€**:
- **Cluster Overview**: í´ëŸ¬ìŠ¤í„° ìƒíƒœ, Worker ëª©ë¡
- **Applications**: ì‹¤í–‰ ì¤‘/ì™„ë£Œëœ ì• í”Œë¦¬ì¼€ì´ì…˜
- **Submit Job**: ìƒˆë¡œìš´ Spark Job ì œì¶œ

---

## 8. í™•ì¥ ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 8.1 ìƒˆë¡œìš´ ì„¼ì„œ íƒ€ì… ì¶”ê°€

#### Step 1: ì„¼ì„œ ë°ì´í„° ìƒì„±ê¸° ìˆ˜ì •

`iot-pipeline/generator/sensor_producer.py`:

```python
# SENSOR_CONFIGSì— ìƒˆ ì„¼ì„œ íƒ€ì… ì¶”ê°€
SENSOR_CONFIGS = {
    "temperature": {...},
    "humidity": {...},
    "pressure": {...},
    "vibration": {  # ìƒˆë¡œìš´ ì„¼ì„œ íƒ€ì…
        "sensors": ["VIB-001", "VIB-002"],
        "unit": "mm/s",
        "min_value": 0.0,
        "max_value": 50.0,
        "threshold_min": 0.0,
        "threshold_max": 30.0
    }
}
```

#### Step 2: Spark ì§‘ê³„ ë¡œì§ í™•ì¸

`iot-pipeline/spark-jobs/pyspark-jobs/batch_aggregation.py`ëŠ” ì„¼ì„œ íƒ€ì…ì— ë¬´ê´€í•˜ê²Œ ë™ì‘í•˜ë¯€ë¡œ ìˆ˜ì • ë¶ˆí•„ìš”.

#### Step 3: í…ŒìŠ¤íŠ¸

```bash
cd generator
poetry run python sensor_producer.py
```

Kafka UIì—ì„œ `vibration` íƒ€ì… ë©”ì‹œì§€ í™•ì¸.

### 8.2 ì§‘ê³„ ì£¼ê¸° ë³€ê²½ (ì‹œê°„ë³„ â†’ ë¶„ë³„)

`batch_aggregation.py` ìˆ˜ì •:

```python
# ê¸°ì¡´: hourly aggregation
df_aggregated = df_with_hour.groupBy("hour", "sensor_type", "location")

# ë³€ê²½: minutely aggregation
df_with_minute = df.withColumn(
    "minute",
    date_format(col("timestamp"), "yyyy-MM-dd HH:mm")
)
df_aggregated = df_with_minute.groupBy("minute", "sensor_type", "location")
```

í…Œì´ë¸” ì´ë¦„ë„ `hourly_stats` â†’ `minutely_stats`ë¡œ ë³€ê²½.

### 8.3 ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

#### Step 1: Schema ì •ì˜

`spark-fastapi/app/schemas/cluster.py`:

```python
class CustomMetric(BaseModel):
    metric_name: str
    value: float
```

#### Step 2: Endpoint êµ¬í˜„

`spark-fastapi/app/api/endpoints/cluster.py`:

```python
@router.get("/metrics", response_model=list[CustomMetric])
async def get_custom_metrics():
    # Spark Master API ë˜ëŠ” PostgreSQL ì¿¼ë¦¬
    return [...]
```

#### Step 3: í…ŒìŠ¤íŠ¸ ì‘ì„±

`spark-fastapi/tests/integration/test_cluster_endpoints.py`:

```python
def test_get_custom_metrics(test_client):
    response = test_client.get("/api/v1/cluster/metrics")
    assert response.status_code == 200
```

### 8.4 í…ŒìŠ¤íŠ¸ ì‹¤í–‰

#### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```bash
cd spark-fastapi
poetry run pytest tests/unit/ -v
```

#### í†µí•© í…ŒìŠ¤íŠ¸

```bash
poetry run pytest tests/integration/ -v
```

#### ì»¤ë²„ë¦¬ì§€ í™•ì¸

```bash
poetry run pytest --cov=app --cov-report=html
firefox htmlcov/index.html  # ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
```

**ì°¸ê³ **: respx ëª¨í‚¹ ì´ìŠˆë¡œ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ skipë©ë‹ˆë‹¤ (`@pytest.mark.respx_issue`).

---

## 9. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 9.1 Kafka ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ**:
```
kafka.errors.NoBrokersAvailable: NoBrokersAvailable
```

**í•´ê²°**:
1. Kafka ì»¨í…Œì´ë„ˆ ì‹¤í–‰ í™•ì¸:
   ```bash
   docker ps | grep kafka
   ```
2. ë„¤íŠ¸ì›Œí¬ í™•ì¸:
   ```bash
   docker network inspect etl-network
   ```
3. /etc/hostsì— `127.0.0.1 kafka-broker-1` ì¶”ê°€

### 9.2 Spark Master ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ**:
```
Exception: Could not connect to Spark Master at spark://localhost:7077
```

**í•´ê²°**:
1. Spark Master ì»¨í…Œì´ë„ˆ ì‹¤í–‰ í™•ì¸
2. Spark Master UI (http://localhost:8080) ì ‘ì† ê°€ëŠ¥í•œì§€ í™•ì¸
3. í¬íŠ¸ 7077 ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸:
   ```bash
   lsof -i :7077
   ```

### 9.3 MinIO S3 ì ‘ê·¼ ì‹¤íŒ¨

**ì¦ìƒ**:
```
org.apache.hadoop.fs.s3a.AWSClientIOException: doesBucketExistV2 on warehouse
```

**í•´ê²°**:
1. MinIO ì»¨í…Œì´ë„ˆ ì‹¤í–‰ í™•ì¸
2. MinIO Consoleì—ì„œ `warehouse` ë²„í‚· ì¡´ì¬ í™•ì¸
3. Access Key/Secret Key í™•ì¸:
   ```bash
   docker logs minio | grep minioadmin
   ```

### 9.4 PostgreSQL JDBC ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ**:
```
org.postgresql.util.PSQLException: Connection refused
```

**í•´ê²°**:
1. PostgreSQL ì»¨í…Œì´ë„ˆ ì‹¤í–‰ í™•ì¸
2. JDBC URL í™•ì¸: `jdbc:postgresql://postgresql:5432/etl_db`
3. ê³„ì • ì •ë³´ í™•ì¸: `etl_user` / `etl_password`
4. ì´ˆê¸°í™” SQL ì‹¤í–‰ ì—¬ë¶€ í™•ì¸:
   ```bash
   docker exec -it postgresql psql -U etl_user -d etl_db -c "\dt iceberg.*"
   ```

### 9.5 Iceberg í…Œì´ë¸”ì´ ë³´ì´ì§€ ì•ŠìŒ

**ì¦ìƒ**:
```
Table iot.hourly_stats not found
```

**í•´ê²°**:
1. Spark Jobì´ ì •ìƒ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë¡œê·¸ í™•ì¸)
2. PostgreSQLì—ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸:
   ```sql
   SELECT * FROM iceberg.iceberg_tables;
   ```
3. MinIOì—ì„œ Parquet íŒŒì¼ í™•ì¸ (MinIO Console â†’ warehouse ë²„í‚·)

### 9.6 FastAPI ì„œë²„ CORS ì—ëŸ¬

**ì¦ìƒ** (ë¸Œë¼ìš°ì € ì½˜ì†”):
```
Access to fetch at 'http://localhost:8000/api/v1/cluster/status' from origin 'http://localhost:5173' has been blocked by CORS policy
```

**í•´ê²°**:
1. `spark-fastapi/app/main.py`ì—ì„œ CORS ì„¤ì • í™•ì¸:
   ```python
   app.add_middleware(
       CORSMiddleware,
       allow_origins=["http://localhost:5173"],
       allow_credentials=True,
       allow_methods=["*"],
       allow_headers=["*"],
   )
   ```
2. FastAPI ì„œë²„ ì¬ì‹œì‘

### 9.7 Docker ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±

**ì¦ìƒ**:
```
Error response from daemon: no space left on device
```

**í•´ê²°**:
```bash
# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì»¨í…Œì´ë„ˆ, ì´ë¯¸ì§€, ë³¼ë¥¨ ì‚­ì œ
docker system prune -a --volumes

# íŠ¹ì • ë³¼ë¥¨ë§Œ ì‚­ì œ
docker volume ls
docker volume rm <volume_name>
```

### 9.8 Kafka ë©”ì‹œì§€ê°€ ì†Œë¹„ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: Spark Jobì„ ì‹¤í–‰í•´ë„ ë°ì´í„°ê°€ ì½íˆì§€ ì•ŠìŒ

**í•´ê²°**:
1. Kafka í† í”½ì— ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸:
   ```bash
   docker exec kafka-broker-1 /opt/kafka/bin/kafka-console-consumer.sh \
     --bootstrap-server kafka-broker-1:29092 \
     --topic sensor-raw \
     --from-beginning \
     --max-messages 1
   ```
2. Consumer Group í™•ì¸:
   ```bash
   docker exec kafka-broker-1 /opt/kafka/bin/kafka-consumer-groups.sh \
     --bootstrap-server kafka-broker-1:29092 \
     --list
   ```
3. Offset ë¦¬ì…‹:
   ```bash
   docker exec kafka-broker-1 /opt/kafka/bin/kafka-consumer-groups.sh \
     --bootstrap-server kafka-broker-1:29092 \
     --group spark-kafka-consumer \
     --reset-offsets --to-earliest --execute --topic sensor-raw
   ```

---

## 10. ê¸°ìˆ  ìŠ¤íƒ ìƒì„¸

### 10.1 ì£¼ìš” ê¸°ìˆ  ë° ë²„ì „

| ê¸°ìˆ  | ë²„ì „ | ëª©ì  | ì„ íƒ ì´ìœ  |
|-----|------|------|----------|
| **Apache Kafka** | 3.9.1 | ë©”ì‹œì§€ ë¸Œë¡œì»¤ | KRaft ëª¨ë“œë¡œ Zookeeper ë¶ˆí•„ìš”, ë†’ì€ ì²˜ë¦¬ëŸ‰ |
| **Apache Spark** | 3.5.3 | ë¶„ì‚° ë°ì´í„° ì²˜ë¦¬ | ì¸ë©”ëª¨ë¦¬ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ ë°°ì¹˜ ì§‘ê³„ |
| **Apache Iceberg** | 1.5.2 | í…Œì´ë¸” í¬ë§· | ìŠ¤í‚¤ë§ˆ ì§„í™”, ACID íŠ¸ëœì­ì…˜ ì§€ì› |
| **PostgreSQL** | 15 | ë©”íƒ€ë°ì´í„° ì¹´íƒˆë¡œê·¸ | Iceberg JDBC ì¹´íƒˆë¡œê·¸ ì €ì¥ |
| **MinIO** | latest | ê°ì²´ ìŠ¤í† ë¦¬ì§€ | S3 í˜¸í™˜ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ |
| **FastAPI** | 0.123.5 | REST API í”„ë ˆì„ì›Œí¬ | ë¹ ë¥¸ ì„±ëŠ¥, ìë™ API ë¬¸ì„œ ìƒì„± |
| **React** | 18 | í”„ë¡ íŠ¸ì—”ë“œ UI | ì»´í¬ë„ŒíŠ¸ ê¸°ë°˜, í’ë¶€í•œ ìƒíƒœê³„ |
| **Python** | 3.11+ | ë°ì´í„° ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ | PySpark, Kafka Producer |
| **TypeScript** | 5.x | í”„ë¡ íŠ¸ì—”ë“œ íƒ€ì… ì•ˆì •ì„± | ëŸ°íƒ€ì„ ì—ëŸ¬ ë°©ì§€ |

### 10.2 ì˜ì¡´ì„± ê´€ë¦¬

#### Python (Poetry)

```bash
# ìƒˆ íŒ¨í‚¤ì§€ ì¶”ê°€
poetry add <package_name>

# ê°œë°œ ì˜ì¡´ì„± ì¶”ê°€
poetry add --group dev <package_name>

# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
poetry update

# ê°€ìƒí™˜ê²½ í™œì„±í™”
poetry shell
```

#### Node.js (npm)

```bash
# ìƒˆ íŒ¨í‚¤ì§€ ì¶”ê°€
npm install <package_name>

# ê°œë°œ ì˜ì¡´ì„± ì¶”ê°€
npm install --save-dev <package_name>

# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
npm update
```

#### Spark (Maven Packages)

```bash
# spark-submit ì‹œ --packages ì˜µì…˜ ì‚¬ìš©
spark-submit --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2
```

### 10.3 ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜

**Docker ë¸Œë¦¿ì§€ ë„¤íŠ¸ì›Œí¬ (`etl-network`)**:

- ëª¨ë“  ì»¨í…Œì´ë„ˆëŠ” ê°™ì€ ë„¤íŠ¸ì›Œí¬ì— ì—°ê²°
- ì»¨í…Œì´ë„ˆ ì´ë¦„ìœ¼ë¡œ í†µì‹  (ì˜ˆ: `kafka-broker-1:29092`)
- í˜¸ìŠ¤íŠ¸ì—ì„œëŠ” `localhost:<í¬íŠ¸>`ë¡œ ì ‘ê·¼

**í¬íŠ¸ ë§¤í•‘**:

```yaml
# docker compose ì˜ˆì‹œ
ports:
  - "9092:9092"   # í˜¸ìŠ¤íŠ¸:ì»¨í…Œì´ë„ˆ
```

---

## 11. ì°¸ê³  ìë£Œ

### 11.1 ê³µì‹ ë¬¸ì„œ

- **Apache Kafka**: https://kafka.apache.org/documentation/
- **Apache Spark**: https://spark.apache.org/docs/latest/
- **Apache Iceberg**: https://iceberg.apache.org/docs/latest/
- **FastAPI**: https://fastapi.tiangolo.com/
- **Poetry**: https://python-poetry.org/docs/
- **MinIO**: https://min.io/docs/minio/linux/index.html
- **PostgreSQL**: https://www.postgresql.org/docs/

### 11.2 í”„ë¡œì íŠ¸ ë‚´ë¶€ ë¬¸ì„œ

- **TESTING.md**: í…ŒìŠ¤íŠ¸ ì „ëµ ë° ì‹¤í–‰ ê°€ì´ë“œ
- **iot-pipeline/README.md**: ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìƒì„¸ ê°€ì´ë“œ
- **spark-fastapi/README.md**: API ì„œë²„ ìƒì„¸ ë¬¸ì„œ
- **spark-fastapi-ui/README.md**: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ ê°€ì´ë“œ

### 11.3 ì£¼ìš” í•™ìŠµ ìë£Œ

- **Kafka ê¸°ì´ˆ**: https://kafka.apache.org/quickstart
- **Spark Structured Streaming**: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
- **Iceberg ì‹œì‘í•˜ê¸°**: https://iceberg.apache.org/docs/latest/getting-started/
- **FastAPI íŠœí† ë¦¬ì–¼**: https://fastapi.tiangolo.com/tutorial/

### 11.4 ì»¤ë®¤ë‹ˆí‹° ë° ì§€ì›

- **Stack Overflow**: `apache-kafka`, `apache-spark`, `apache-iceberg`, `fastapi` íƒœê·¸
- **GitHub Issues**:
  - Kafka: https://github.com/apache/kafka/issues
  - Spark: https://github.com/apache/spark/issues
  - Iceberg: https://github.com/apache/iceberg/issues
  - FastAPI: https://github.com/tiangolo/fastapi/issues

---

## ğŸ“Œ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì¸ìˆ˜ì¸ê³„ ì™„ë£Œ í™•ì¸)

- [ ] Docker í™˜ê²½ êµ¬ì¶• ì™„ë£Œ (ëª¨ë“  ì»¨í…Œì´ë„ˆ ì‹¤í–‰)
- [ ] Kafka í† í”½ ìƒì„± ë° ë©”ì‹œì§€ í™•ì¸
- [ ] Spark ë°°ì¹˜ Job ì‹¤í–‰ ì„±ê³µ
- [ ] Iceberg í…Œì´ë¸” ì¡°íšŒ ê°€ëŠ¥
- [ ] FastAPI ì„œë²„ ì‹¤í–‰ ë° API í˜¸ì¶œ ì„±ê³µ
- [ ] ì›¹ UI ì ‘ì† ë° í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ í™•ì¸
- [ ] ê° ì„œë¹„ìŠ¤ ë¡œê·¸ í™•ì¸ ë°©ë²• ìˆ™ì§€
- [ ] íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ìˆ™ì§€
- [ ] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ì»¤ë²„ë¦¬ì§€ í™•ì¸

---

## ğŸ”— ì—°ë½ì²˜

**ì§ˆë¬¸ ë˜ëŠ” ì´ìŠˆ ë°œìƒ ì‹œ**:
- í”„ë¡œì íŠ¸ GitHub Repository: (URL ì¶”ê°€ í•„ìš”)
- ë‹´ë‹¹ì ì´ë©”ì¼: (ì´ë©”ì¼ ì¶”ê°€ í•„ìš”)

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-12-11